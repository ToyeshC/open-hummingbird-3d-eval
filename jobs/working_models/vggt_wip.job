#!/bin/bash
#SBATCH --job-name=eval_mvimgnet_vggt
#SBATCH --output=logs/working_models/eval_mvimgnet_vggt_%j.log
#SBATCH --error=logs/working_models/eval_mvimgnet_vggt_%j.log
#SBATCH --partition=gpu_a100
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gpus=4
#SBATCH --cpus-per-task=64
#SBATCH --time=00:05:00

# ToDo: Remember to change the job name and output file names to match the parameters

echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

module purge
module load 2023
module load Miniconda3/23.5.2-0

# Activate the environment
# source activate hbird
# Use the conda 'hook' so activation works in non-interactive SLURM jobs.
# 'source activate' is deprecated and may fail; this is the modern, reliable way.
eval "$(/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda shell.bash hook)"
conda activate hbird

cd $HOME/open-hummingbird-eval

srun python python_scripts/run_eval.py \
  --model_repo facebook/VGGT-1B \
  --model_name vggt-1b \
  --batch_size 64 \
  --input_size 504 \
  --patch_size 14 \
  --d_model 1536 \
  --nn_method faiss \
  --n_neighbours 30 \
  --augmentation_epoch 1 \
  --dataset_name mvimgnet \
  --data_dir ./datasets/split_angles_mvimagenet \
  --train_bins 90 \
  --val_bins 0 \
  --num_workers 32

# Notes:
# - d_model is bigger for vggt
# - The model accepts any input_size