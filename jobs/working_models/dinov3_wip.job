#!/bin/bash
#SBATCH --job-name=eval_mvimgnet_dino_v3
#SBATCH --output=logs/working_models/eval_mvimgnet_dino_v3_%j.log
#SBATCH --error=logs/working_models/eval_mvimgnet_dino_v3_%j.log
#SBATCH --partition=gpu_a100
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=8
#SBATCH --time=00:05:00

# ToDo: Remember to change the job name and output file names to match the parameters

echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

module purge
module load 2023
module load Miniconda3/23.5.2-0

# export HF_TOKEN='hf_xxx'
# export HUGGINGFACE_HUB_TOKEN="$HF_TOKEN"
# # sanity check
# python - <<'PY'
# from huggingface_hub import whoami
# import os
# print("HF user:", whoami(token=os.environ.get("HUGGINGFACE_HUB_TOKEN","<none>"))["name"])
# PY


# Read Hugging Face token from .env. It should contain:
# HUGGINGFACE_HUB_TOKEN=hf_xxx
if [ -f ".env" ]; then
  set -a
  . .env
  set +a
fi

eval "$(/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda shell.bash hook)"
conda activate hbird

cd $HOME/open-hummingbird-eval

srun python python_scripts/run_eval.py \
  --model_repo facebook/dinov3-vitb16-pretrain-lvd1689m \
  --model_name "" \
  --batch_size 64 \
  --input_size 512 \
  --patch_size 16 \
  --d_model 768 \
  --nn_method faiss \
  --n_neighbours 30 \
  --augmentation_epoch 1 \
  --dataset_name mvimgnet \
  --data_dir ./datasets/split_angles_mvimagenet \
  --train_bins 90 \
  --val_bins 0 \
  --num_workers 8

# Notes:
# - Input size should be 512 (model was trained on this resolution).
#   For Dino V3 input_size=224 would also work, but with misaligned embeddings (DINOv2 models allow this silently).
#   This will lead to a drop in performance. To avoid this the embeddings should be interpolated.
#   We want to use input_size=512 for consistency.